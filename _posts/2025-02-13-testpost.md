---
title: "GPT & BERT : Principal of transformer and huggingface"
excerpt: "ref. Bert와 GPT로 배우는 자연어 처리 : 트랜스포머 핵심원리와 허깅페이스 패키지 활용법 (저자 : 이기창) "

categories: # 카테고리 설정
  - categories1
tags: # 포스트 태그
  - [tag1, tag2]

permalink: /categories1/GPT & BERT Principal of transformer and huggingface/ # 포스트 URL

toc: true # 우측에 본문 목차 네비게이션 생성
toc_sticky: true # 본문 목차 네비게이션 고정 여부

date: 2025-02-17 # 작성 날짜
last_modified_at: 2025-02-17 # 최종 수정 날짜
---

# Bert와 GPT로 배우는 자연어 처리 : 트랜스포머 핵심원리와 허깅페이스 패키지 활용법

<aside>
💡 도서명 : Bert와 GPT로 배우는 자연어 처리 : 트랜스포머 핵심원리와 허깅페이스 패키지 활용법 (저자 : 이기창)

</aside>

# 1 딥러닝 기반 자연어 처리 모델

자연어처리 모델은 자연어를 입력받아서 해당 입력이 특정 범주일 확률을 반환하는 확률 함수다… 이하 생략

# 2 트랜스퍼러닝

특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용

- 업스트림 태스크 - 말뭉치의 문맥을 이해하는 과제
    - 다음 단어 맞히기
    - 빈칸 채우기
- 다운스트림 태스크 - 자연어처리의 구체적 문제
    - 문서 분류
    - 개체명 인식
    

### 업스트림 태스크

- 대규모 말뭉치를 활용해 다음에 올 단어의 확률 조정
    - ex)  티끌 모아 — 일 경우 —에 들어가는 단어 ‘태산’의 확률을 높이는 방식으로 학습
- 빈칸채우기 (Bert 계열 모델은 이 방법으로 pre train)
    - ex) 티끌 — 태산 빈칸 채우기를 반복 학습하면 빈칸 채우기 가능.
    - 빈칸채우기로 업스트림 태스크를 수행한 모델을 `마스크 언어 모델` 이라고 함.
    

→ 업스트림 태스크의 경우 정답(label)이 필요한 `supervised learning`과 달리 수작업 없이 학습 데이터를 학습 가능 (비용 측면에서 이득). 이처럼 데이터 내에서 정답을 만들고 이를 바탕으로 모델을 학습하는 방법을 `self-supervised learning`이라고 함.

### 다운스트림 태스크

업스트림 태스크로 `pre-train` → 다운스트림 태스크 해결

- `fine-tuning` : `pre-train` 을 마친 모델을 다운스트림 태스크에 맞게 업데이트
    - 문서분류 (4장)
        - 입력문장이 긍정, 중립, 부정 따위의 어떤 범주에 속하는지 확률값 반환
    - 자연어추론 (5장)
        - 입력문장 2개 사이의 관계가 참, 거짓, 중립 등 어떤 범주에 속하는지 확률값 반환
    - 개체명 인식 (6장)
        - 입력문장이 단어별로 기관명, 인명, 지명 등 어떤 개체명 범주에 속하는지 확률값 반환
    - 질의 응답 (7장)
        - 질문과 지문을 입력받아 각 단어가 정답의 시작일 확률과 끝일 확률값 반환
    - 문장 생성 (8장)
        - 자연어를 입력받고 어휘 전체에 대한 확률값 반환 (입력 문장 다음에 올 단어로 얼마나 적절한지 나타냄)

- `fine-tuning` 이외에도 `prompt tuning`, `in-context learning` 으로 다운스트림 태스크 학습 가능
    - `fine-tuning` 으로 모델 전체를 업데이트하려면 많은 비용이 들어가므로 다른 방식도 주목받음
    - 특히 `in-comtext learning`의 경우 모델을 업데이트하지 않고 다운스트림 데스크 데이터를 몇 건 참고하느냐의 차이를 보임
        - 제로샷 러닝
        - 원샷 러닝
        - 퓨샷 러닝
        

---

# 3 학습 파이프라인

1. 각종 설정값 정하기
    - 프리트레인 모델 선택
    - 데이터(코퍼스) 선택
    - 저장경로 설정
    - 하이퍼파라미터 설정
        - `learning rate`
        - `batch size`
2. 데이터 내려받기
    - 1에서 설정한 args에 따라 가져오는거라 걍 있는 코드 사용하면 될듯 ex) korpora 패키지 사용
3. 프리트레인을 마친 모델 준비하기
    - [github.com/huggingface/transformers](http://github.com/huggingface/transformers)
    - [github.com/Beomi/KcBERT](http://github.com/Beomi/KcBERT)
4. 토크나이저 준비하기
    - 문장, 형태소 등 단위로 나눔
    - `token sequence`로 분석하는 과정을 `tokenization`이라고 하고, 이를 수행하는 프로그램을 `tokenizer`라고 함.
        - 토크나이저의 알고리즘 종류 - `BPE` , `wordpiece` 등
5. 데이터 로더 준비하기
    - 인스턴스를 뽑아 `batch` 구성
    - `collate`
        - `batch`는 tensor로,  모양이 고정적이어야 하므로 길이를 맞추는 것. 가장 긴 길이로 맞춘다면 짧은 인스턴스의 길이가 늘어남 (ex) 1,234,5,0,0,0)
        - list에서 tensor로 자료형을 변환하는 과정도 `collate`에 포함.
6. 태스크 정의하기
    - 이 책에서는 `pytorch lightening` 라이브러리 사용
    - 모델학습은 배치단위로 이뤄지므로 배치를 모델에 입력한 뒤 모델 출력을 정답과 비교해 차이 계산, 그 차이를 최소화 하는 것 `optimizing` , 이 일련의 과정을 `step`이라고 함
7. 모델 학습하기
    - `pytorch lightening`의 경우 `trainer` 객체가 GPU설정, 학습 기록 로깅, 체크포인트 저장 등 설정 자동으로 저장해줌
    

---

# 4 개발환경설정

google colab을 사용한다. 이하 생략.

---

# 5 토큰화

대표적인 한국어 토크나이저 : 은전한닢 `mecab`, 꼬꼬마 `kkma` 등..

### 단어 단위 토큰화

- 어제 카페 갔었어 → 어제, 카페, 갔었어
- 단점
    - 공백으로 분리하면 토크나이저를 사용하지 않아도 되나, 어휘 집합(vocabulary)의 크기가 너무 커짐
    - `mecab`으로 토큰화하면 의미 단위 (갔었)으로 토큰화해 급격하게 어휘 집합이 커지는 것을 막을 수 있으나 한계가 있음

### 문자 단위 토큰화

- 어제 카페 갔었어 → 어, 제, 카, 페, 갔, 었, 어
- 한글로 표현할 수 있는 글자는 11,172개. 알파벳과 숫자를 고려해도 약 15,000개므로 `미등록토큰` 문제에서 자유로움
    - `미등록토큰`이란 어휘 집합(vocabulary)에 없는 토큰으로, 단어 단위 토큰화 시 신조어 등에서 문제가 발견됨
- 단점
    - 의미 단위가 되기 어려움
    - 토큰 시퀀스의 길이가 길어짐

### 서브워드 단위 토큰화

- 단어/ 문자 토큰화 중간 형태 토큰화로, `BPE(byte pair incoding)` 기법이 이에 해당함.
    - `BPE(byte pair incoding)`
        - GPT가 `BPE`로 토큰화 수행하며, BERT또한 이와 유사한 `wordpiece` 사용
        - aaabdaaabac → ZabdZabac → ZYdZYac →XdXac
            - 위 예제의 경우 vocabulary크기가 4였으나 BPE 수행 이후 7개로 늘어났지만, 데이터 길이가 5로 줄어 데이터를 효율적으로 압축 가능
            - 즉, 사전 크기 증가를 억제하면서도 정보를 효율적으로 압축한 것
            - `BPE` 어휘 집합을 구축하는 절차
                1. 코퍼스의 모든 문장을 공백으로 나눠줌 (프리토크나이즈)
                2. 바이그램 반복, 어휘집합 업데이트
                3. 병합 우선순위 도출
                4. 미등록토큰<unk>를 포함해 토큰화
    - `wordpiece`
        - `BPE` 와 비슷하게 자주 등장한 문자열을 토큰으로 인식
        - 차이점 : 빈도 기준 병합이 아니라 우도(likelihood)를 높이는 쌍 병합
    

---

# 6 어휘집합구축

1. 환경구축
2. 말뭉치 다운로드 및 전처리
3. GPT 토크나이저 구축
4. BERT 토크나이저 구축

---

# 7 토큰화

이하생략

---

# 8 언어 모델

단어 시퀀스에 확률을 부여하는 것으로, 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력하는 것

- 조건부 확률 이해
    - $p(w1, w2, w3) = P(w1) * P(w2|w1) * P(w3|w1,w2)$수
- 순방향 언어모델 (GPT, ELmo)
- 역방향 언어모델 (ELmo)
- 마스크 언어모델 (BERT) - 양방향
- 스킵 그램 모델 - 특정 범위를 정해두고 어떤 단어가 들어올지 분류 (Word2Vec)

---

# 9 트랜스포머

2017년 구글이 제안한 sequence-to-sequence 모델. 이 모델은 특정 속성을 지닌 시퀀스를 다른 속성의 시퀀스로 변환하는 작업 수행

### 인코더와 디코더

- 인코더가 소스 시퀀스 압축, 디코더가 타깃 시퀀스 생성

### 셀프 어텐션

- 기존 모델의 단점
    - convolution filter크기에 따라 제한적이라는 CNN의 단점
    - 시퀀스 길이가 길어질 수록 정보가 사라지는 RNN의 단점
- 위 두 단점을 보완해, 각 단어가 어떤 단어와 연관이 큰지(weighted sum 사용)를 블록(레이어)별로 계산
- 개별 단어 + 전체 입력 시퀀스를 대상으로 계산하므로 지엽적이라는 CNN의 단점과 단어를 잊어버리는 RNN의 단점을 보완

- 동작원리
    - input layer (입력 임베딩 + 위치정보)
